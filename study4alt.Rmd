---
title: "Study 4 Alternative Analysis"
author: "Luis P. Prieto"
abstract: "This is a cleaner, simpler analysis of the data (including complete coding of the video in terms of Activity, Social plane and Experimental condition) of JDC2015 study (study 4, in the paper)."
output: html_document
---


```{r, echo=FALSE, message=FALSE, warning=FALSE}
source("./lib/rollingWindows.R")
source("./lib/loadIndex.R")
source("./lib/extremeLoadExtraction.R")
source("./lib/aggregateEpisodeData.R")
source("./lib/multiplot.R")
source("./lib/outliers.R")
source('./lib/codeAnalyses.R')
source('./lib/generateVideoSnippetScript.R')

loaddata <- get(load("./data/study4/study4LoadData.Rda"))
str(loaddata)
```

## Exploratory analysis

### Distribution of values

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.width=5, eval=FALSE}
library(ggplot2)

#ggplot(data=loaddata, aes(x=loaddata$value.Mean))+geom_density()
#ggplot(data=loaddata, aes(x=loaddata$value.SD))+geom_density()
#ggplot(data=loaddata, aes(x=loaddata$value.Sac))+geom_density()
#ggplot(data=loaddata, aes(x=loaddata$value.Fix))+geom_density()
#ggplot(data=loaddata, aes(x=loaddata$value.Theta))+geom_density()
#ggplot(data=loaddata, aes(x=loaddata$value.Jerk.Mean))+geom_density()

ggplot(data=loaddata, aes(x=loaddata$value.Mean.norm))+geom_density()
ggplot(data=loaddata, aes(x=loaddata$value.SD.norm))+geom_density()
ggplot(data=loaddata, aes(x=loaddata$value.Sac.norm))+geom_density()
ggplot(data=loaddata, aes(x=loaddata$value.Fix.norm))+geom_density()
ggplot(data=loaddata, aes(x=loaddata$value.Theta.norm))+geom_density()
ggplot(data=loaddata, aes(x=loaddata$value.Jerk.Mean.norm))+geom_density()

```

**Q**: The densities are similar, but of course the scaling is different... which one should we use?

### Correlations between load metrics

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.width=5}
library(corrplot)
#M <- cor(loaddata[, c(3:6,11:12)])
#corrplot.mixed(M,main="Correlations, raw values")
M <- cor(loaddata[, c(14,17,20,23,29,35)])
corrplot.mixed(M,main="Correlations, normalized values")
```

**Q**: The correlations vary a lot depending on whether we use the normalized or raw values... which ones should we use?

### PCA

```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.height=5, fig.width=5}
library(FactoMineR)

#Overall for the four sessions
res.pca = PCA(loaddata[, c(3:6,11:12)], scale.unit=TRUE, ncp=5, graph=F)
plot.PCA(res.pca, axes=c(1, 2), choix="var", title="Raw data, dims 1/2")
plot.PCA(res.pca, axes=c(3, 4), choix="var", title="Raw data, dims 3/4")
loaddata$Dim1 = res.pca$ind$coord[,1]
loaddata$Dim2 = res.pca$ind$coord[,2]
loaddata$Dim3 = res.pca$ind$coord[,3]
loaddata$Dim4 = res.pca$ind$coord[,4]

res.pca.norm = PCA(loaddata[, c(14,17,20,23,29,35)], scale.unit=TRUE, ncp=5, graph=F)
plot.PCA(res.pca.norm, axes=c(1, 2), choix="var", title="Normalized data, dims 1/2")
plot.PCA(res.pca.norm, axes=c(3, 4), choix="var", title="Normalized data, dims 3/4")
loaddata$Dim1Norm = res.pca.norm$ind$coord[,1]
loaddata$Dim2Norm = res.pca.norm$ind$coord[,2]
loaddata$Dim3Norm = res.pca.norm$ind$coord[,3]
loaddata$Dim4Norm = res.pca.norm$ind$coord[,4]
```


Components (raw data): 

1. Aggregate of most ET (except Fix) and Acc metrics 
2. Aggregate of most ET (except Sac) metrics
3. Inverse of EEG-measured load (theta)
4. Saccades vs. Jerk

Components (normalized data): 
1. Saccades, Pupil SD and Acc Jerk (and Theta-based)
2. Fixations vs. Pupil Mean and Theta-based
3. Acc Jerk vs. Theta-based and Fixations
4. Fixations and Acc Jerk


### HMM

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
library(depmixS4)
library(ggplot2)

## We plot the BIC for different number of states, just to check
# states <- 2:16
# BICs <- 39000
# for(i in states){
#     print(paste(i,"..."))
#     # Overall
#     set.seed(1)
#     #set up the HMM model
#     #mod <- depmix(list(value.Mean.norm~1,value.SD.norm~1,value.Sac.norm~1,value.Fix.norm~1,value.Jerk.Mean.norm~1,value.Theta.norm~1), data=loaddata, nstates=i, family=list(gaussian(),gaussian(),gaussian(),poisson(),gaussian(),gaussian()),ntimes=summary(as.factor(loaddata$session)), verbose=F)
#     mod <- depmix(list(value.Mean~1,value.SD~1,value.Sac~1,value.Fix~1,value.Jerk.Mean~1,value.Theta~1), data=loaddata, nstates=i, family=list(gaussian(),gaussian(),gaussian(),poisson(),gaussian(),gaussian()),ntimes=summary(as.factor(loaddata$session)), verbose=F)
#     #fit the model 
#     f <- fit(mod, verbose=F)
# #    print(f)
#     BICs[i] <- BIC(f)
#     esttrans <- posterior(f)
#     loaddata$MMstate <- esttrans[,1]
#     print(table(loaddata$session,loaddata$MMstate))
# }
# BICs[1] <- max(BICs[2:16])
# qplot(1:16,BICs,geom="line")

#numstates <- 8 # The criteria should also be that all sessions have all states... no number of states complies with this in the normalized metrics !!!! We try the HMM with raw data then...
numstates <- 6 # This is the smallest number that is close to the minimum in BIC 
# Overall
set.seed(1)
#set up the HMM model
mod <- depmix(list(value.Mean~1,value.SD~1,value.Sac~1,value.Fix~1,value.Jerk.Mean~1,value.Theta~1), data=loaddata, nstates=numstates, family=list(gaussian(),gaussian(),gaussian(),poisson(),gaussian(),gaussian()),ntimes=summary(as.factor(loaddata$session)), verbose=F)
#fit the model 
f <- fit(mod, verbose=F)
#check to see how the state transtion matricies and process mean/sd matches our sample data
summary(f)
#get the estimated state for each timestep 
esttrans <- posterior(f)
loaddata$MMstate6 <- esttrans[,1]
table(loaddata$session,loaddata$MMstate6)
plot(1:nrow(loaddata), esttrans[,1], type='p', col=as.factor(loaddata$session), main=paste('Estimated state',numstates))

```

The load states seem to be the following:

1. Hi-PDmean, MidHi-PDsd, Hi-Sac, Hi-Fix, Hi-Jerk, Hi-Theta: High CL (except EEG), High physical
2. Hi-PDmean, Hi-PDsd, Hi-Sac, Mid-Fix, Low-Jerk, Mid-Theta: High CL, Low physical
3. MidLo-PDmean, Low-PDsd, Low-Sac, Hi-Fix, Low-Jerk, Low-Theta: Low CL (except EEG), Low physical
4. Highest-PDmean, Hi-PDsd, Mid-Sac, Mid-Fix, Mid-Jerk, Low-Theta: High CL (PD and EEG), Mid physical
5. MidLo-PDmean, Mid-PDsd, MidHi-Sac, Hi-Fix, MidHi-Jerk, Mid-Theta: Mid CL, mid physical
6. Lo-PDmean, Low-PDsd, Mid-Sac, Mid-Fix, Low-Jerk, Hi-Theta: Low CL (PD and EEG), Low physical


## How do cognitive load-related METRICS relate to the MANIPULATION (with/without helper)?

### Separate load-related metrics

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
library(gplots)

# Pupil mean
plotmeans(value.Mean~value.Experimental,data=loaddata,connect=F) # The metric behaves as expected...
t.test(loaddata[loaddata$value.Experimental=="C1","value.Mean"],loaddata[loaddata$value.Experimental=="C2","value.Mean"])$p.value #... but the difference is not significant
plotmeans(value.Mean.norm~value.Experimental,data=loaddata,connect=F) # The metric behaves in the opposite way!
t.test(loaddata[loaddata$value.Experimental=="C1","value.Mean.norm"],loaddata[loaddata$value.Experimental=="C2","value.Mean.norm"])$p.value #... but the difference IS significant

# Pupil sd
plotmeans(value.SD~value.Experimental,data=loaddata,connect=F) # The metric behaves opposite to expected
t.test(loaddata[loaddata$value.Experimental=="C1","value.SD"],loaddata[loaddata$value.Experimental=="C2","value.SD"])$p.value #... but the difference is not significant
plotmeans(value.SD.norm~value.Experimental,data=loaddata,connect=F) # The metric behaves as expected...
t.test(loaddata[loaddata$value.Experimental=="C1","value.SD.norm"],loaddata[loaddata$value.Experimental=="C2","value.SD.norm"])$p.value #... and the difference is significant!

# Saccade Spd
plotmeans(value.Sac~value.Experimental,data=loaddata,connect=F) # The metric behaves as expected...
t.test(loaddata[loaddata$value.Experimental=="C1","value.Sac"],loaddata[loaddata$value.Experimental=="C2","value.Sac"])$p.value #... and the difference is significant!
plotmeans(value.Sac.norm~value.Experimental,data=loaddata,connect=F) # The metric behaves as expected
t.test(loaddata[loaddata$value.Experimental=="C1","value.Sac.norm"],loaddata[loaddata$value.Experimental=="C2","value.Sac.norm"])$p.value #... and the difference is significant!

# Long fixations
plotmeans(value.Fix~value.Experimental,data=loaddata,connect=F) # The metric behaves opposite as expected...
t.test(loaddata[loaddata$value.Experimental=="C1","value.Fix"],loaddata[loaddata$value.Experimental=="C2","value.Fix"])$p.value #... but the difference is not significant

# Physical Jerk
plotmeans(value.Jerk.Mean~value.Experimental,data=loaddata,connect=F) # The metric behaves as expected...
t.test(loaddata[loaddata$value.Experimental=="C1","value.Jerk.Mean"],loaddata[loaddata$value.Experimental=="C2","value.Jerk.Mean"])$p.value #... but the difference is not significant
plotmeans(value.Jerk.Mean.norm~value.Experimental,data=loaddata,connect=F) # The metric behaves as expected
t.test(loaddata[loaddata$value.Experimental=="C1","value.Jerk.Mean.norm"],loaddata[loaddata$value.Experimental=="C2","value.Jerk.Mean.norm"])$p.value #... and the difference IS significant!

# EEG Theta
plotmeans(value.Theta~value.Experimental,data=loaddata,connect=F) # The metric behaves opposite as expected...
t.test(loaddata[loaddata$value.Experimental=="C1","value.Theta"],loaddata[loaddata$value.Experimental=="C2","value.Theta"])$p.value #... but the difference IS significant!
plotmeans(value.Theta.norm~value.Experimental,data=loaddata,connect=F) # The metric behaves in the opposite way!
t.test(loaddata[loaddata$value.Experimental=="C1","value.Theta.norm"],loaddata[loaddata$value.Experimental=="C2","value.Theta.norm"])$p.value #... but the difference IS significant!

```

We see that the normalized PupilSD, (normalized or not) Sac, normalized Jerk behave as expected and ARE SIGNIFICANTLY different. Jerk and saccade speed can be explained by the physical distribution of the manipulation

However, we also find that the EEG Theta (normalized or not), and normalized PupilMean gives OPPOSITE results to expected, in a significant way!

**Q**: What do we make out of this? That only some metrics are good? that the goodness depends on the task at hand (this would have to be checked against the behavioral variables -- see below)?

### Multi-metric load indices. Is it better than single ones (i.e., does it distinguish experimental condition)? 

#### 4 Eyetracking

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}

loaddataET <- calculateCoarseFineLoadIndex(loaddata,c(3:6),normalize=T) # We ensure that the values are normalized for those of the first window in which little load is assumed (just putting the eyetracker on)
plotmeans(CoarseLoad ~ value.Experimental, data=loaddataET, connect=F) # Does not behave as expected
t.test(loaddataET[loaddataET$value.Experimental=="C1","CoarseLoad"],loaddataET[loaddataET$value.Experimental=="C2","CoarseLoad"])$p.value #... but the difference is not significant
plotmeans(FineLoad ~ value.Experimental, data=loaddataET, connect=F) # Does not behave as expected
t.test(loaddataET[loaddataET$value.Experimental=="C1","FineLoad"],loaddataET[loaddataET$value.Experimental=="C2","FineLoad"])$p.value #... but the difference is not significant

```

We see that the eyetrack-based load index does not catch the expectd difference in load!

#### 4 Eyetracking + Theta


```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}

loaddataETT <- calculateCoarseFineLoadIndex(loaddata,c(3:6),normalize=T, inversecols = 11) # We ensure that the values are normalized for those of the first window in which little load is assumed (just putting the eyetracker on)
plotmeans(CoarseLoad ~ value.Experimental, data=loaddataETT, connect=F) # Does not behave as expected
t.test(loaddataETT[loaddataETT$value.Experimental=="C1","CoarseLoad"],loaddataETT[loaddataETT$value.Experimental=="C2","CoarseLoad"])$p.value #... but the difference is not significant
plotmeans(FineLoad ~ value.Experimental, data=loaddataETT, connect=F) # Does not behave as expected
t.test(loaddataETT[loaddataETT$value.Experimental=="C1","FineLoad"],loaddataETT[loaddataETT$value.Experimental=="C2","FineLoad"])$p.value #... but the difference is not significant

```

Adding the EGG does not help either in catching the expected difference in load!

#### 4 Eyetracking + Theta + Acc Jerk


```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}

loaddataETTA <- calculateCoarseFineLoadIndex(loaddata,c(3:6,12),normalize=T, inversecols = 11) # We ensure that the values are normalized for those of the first window in which little load is assumed (just putting the eyetracker on)
plotmeans(CoarseLoad ~ value.Experimental, data=loaddataETTA, connect=F) # Does not behave as expected
t.test(loaddataETTA[loaddataETTA$value.Experimental=="C1","CoarseLoad"],loaddataETTA[loaddataETTA$value.Experimental=="C2","CoarseLoad"])$p.value #... but the difference is not significant
plotmeans(FineLoad ~ value.Experimental, data=loaddataETTA, connect=F) # Does not behave as expected
t.test(loaddataETTA[loaddataETTA$value.Experimental=="C1","FineLoad"],loaddataETTA[loaddataETTA$value.Experimental=="C2","FineLoad"])$p.value #... but the difference is not significant

```

Adding the physical load does not help either in catching the expected difference in load!

This means that, even if some of the measures do catch the difference, others do not (or do it opposingly, hence canceling the effect of the "good metrics")


### PCA dimensions

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
#Raw dimensions
plotmeans(Dim1 ~ value.Experimental, data=loaddata, connect=F) # Behaves as expected
t.test(loaddata[loaddata$value.Experimental=="C1","Dim1"],loaddata[loaddata$value.Experimental=="C2","Dim1"])$p.value #... and the difference is significant!
plotmeans(Dim2 ~ value.Experimental, data=loaddata, connect=F) # Behaves opposite as expected
t.test(loaddata[loaddata$value.Experimental=="C1","Dim2"],loaddata[loaddata$value.Experimental=="C2","Dim2"])$p.value #... and the difference is significant!
plotmeans(Dim3 ~ value.Experimental, data=loaddata, connect=F) # Behaves opposite as expected (it is theta-based)
t.test(loaddata[loaddata$value.Experimental=="C1","Dim3"],loaddata[loaddata$value.Experimental=="C2","Dim3"])$p.value #... and the difference is significant
plotmeans(Dim4 ~ value.Experimental, data=loaddata, connect=F) # Behaves as expected
t.test(loaddata[loaddata$value.Experimental=="C1","Dim4"],loaddata[loaddata$value.Experimental=="C2","Dim4"])$p.value #... but the difference is not significant

#Normalized dimensions
plotmeans(Dim1Norm ~ value.Experimental, data=loaddata, connect=F) # Behaves as expected
t.test(loaddata[loaddata$value.Experimental=="C1","Dim1Norm"],loaddata[loaddata$value.Experimental=="C2","Dim1Norm"])$p.value #... and the difference is significant!
plotmeans(Dim2Norm ~ value.Experimental, data=loaddata, connect=F) # Not clear how it should behave... goes more into Fix
t.test(loaddata[loaddata$value.Experimental=="C1","Dim2Norm"],loaddata[loaddata$value.Experimental=="C2","Dim2Norm"])$p.value #... and the difference is significant!
plotmeans(Dim3Norm ~ value.Experimental, data=loaddata, connect=F) # Not clear how it should behave... goes more into Jerk
t.test(loaddata[loaddata$value.Experimental=="C1","Dim3Norm"],loaddata[loaddata$value.Experimental=="C2","Dim3Norm"])$p.value #... and the difference is significant
plotmeans(Dim4Norm ~ value.Experimental, data=loaddata, connect=F) # Behaves as expected
t.test(loaddata[loaddata$value.Experimental=="C1","Dim4Norm"],loaddata[loaddata$value.Experimental=="C2","Dim4Norm"])$p.value #... but the difference is not significant

```

Interestingly, in both normalized and raw data, the first dimension behaves as expected, and the differences are significant!


### HMMs

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}

expdata <- loaddata[loaddata$value.Experimental=="C1" | loaddata$value.Experimental=="C2",]
expdata$value.Experimental <- factor(expdata$value.Experimental)
tabhmm <- table(expdata$value.Experimental,expdata$MMstate6)
chisq.test(tabhmm)$p.value
chisq.test(tabhmm)$residuals

```

The 6-state HMMs do not catch significant differences in the experimental conditions.


## How do cognitive load-related METRICS relate to the orchestration dimensions? What are the orchestration load patterns?

### Separate load-related metrics

We can try to discern the effect of the Activity and Social level on the different metrics, and see whether, after substracting those effects, the metric still catches the difference between C1 and C2

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
library(aod)

# Orchestration patterns
lmMn <- lm(value.Mean ~ value.Activity+value.Social, data=loaddata)
summary(lmMn) # TDT, EXP, CLS are high load, MON, REP, GRP are low load
anova(lmMn)
lmMnN <- lm(value.Mean.norm ~ value.Activity+value.Social, data=loaddata)
summary(lmMnN) # TDT, CLS are high load, MON, REP, GRP are low load
anova(lmMnN)
# Models based on all the data -- influence of the experimental condition?
lmMean <- lm(value.Mean ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmMean) # C1 and C2 have coefficients opposite as expected...
anova(lmMean)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmMean), Sigma = vcov(lmMean), L = l)  # ... And the difference is significant!
lmMeanNorm <- lm(value.Mean.norm ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmMeanNorm)  # C1 and C2 have coefficients opposite as expected...
anova(lmMeanNorm)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmMeanNorm), Sigma = vcov(lmMeanNorm), L = l)  # ... And the difference is significant!

```

* **Pupil mean pattern**: Activity and Social are significant, TDT, EXP, CLS are high load, MON, REP, GRP are low load
* **(norm) Pupil mean pattern**: Activity and Social are significant, TDT, CLS are high load, MON, REP, GRP are low load
* **Contrasts**: In both cases, the model puts load coefficients in the wrong direction, significantly!


```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmSd <- lm(value.SD ~ value.Activity+value.Social, data=loaddata)
summary(lmSd) # QUE (maybe TDT) are high load, GRP is low load
anova(lmSd)
lmSdN <- lm(value.SD.norm ~ value.Activity+value.Social, data=loaddata)
summary(lmSdN) # QUE, CLS are high load, MON, GRP are low load
anova(lmSdN)
# Models based on all the data -- influence of the experimental condition?
lmSDx <- lm(value.SD ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmSDx) # C1 and C2 have coefficients opposite as expected...
anova(lmSDx)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmSDx), Sigma = vcov(lmSDx), L = l)  # ... And the difference is not significant
lmSDNorm <- lm(value.SD.norm ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmSDNorm)  # C1 and C2 have coefficients as expected
anova(lmSDNorm)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmSDNorm), Sigma = vcov(lmSDNorm), L = l)  # ... And the difference is significant!

```

* **Pupil sd pattern**: Activity and Social are significant, QUE, TDT, CLS are high load, GRP are low load
* **(norm) Pupil sd pattern**: Activity and Social are significant, QUE, CLS are high load, MON, GRP are low load
* **Contrasts**: In the raw metric, the C1/C2 coefficients are not so different, but in the normalized SD metric the coefficients are as expected, and the difference is significant!



```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmSc <- lm(value.Sac ~ value.Activity+value.Social, data=loaddata)
summary(lmSc) # QUE (maybe TDT) are high load, GRP is low load
anova(lmSc)
lmScN <- lm(value.Sac.norm ~ value.Activity+value.Social, data=loaddata)
summary(lmScN) # QUE, CLS are high load, MON, GRP are low load
anova(lmScN)
# Models based on all the data -- influence of the experimental condition?
lmSac <- lm(value.Sac ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmSac) # C1 and C2 have coefficients as expected...
anova(lmSac)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmSac), Sigma = vcov(lmSac), L = l)  # ... And the difference is significant!
lmSacNorm <- lm(value.Sac.norm ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmSacNorm)  # C1 and C2 have coefficients as expected
anova(lmSacNorm)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmSacNorm), Sigma = vcov(lmSacNorm), L = l)  # ... And the difference is significant!

```

* **Saccade Spd pattern**: Activity is NOT significant, Social IS. CLS is higher load, GRP lower load
* **(norm) Saccade Spd pattern**: Activity IS significant, and Social is NOT. TDT, EXP is high load, MON, REP are low load
* **Contrasts**: Both in the raw and normalized Sac metric, the C1/C2 coefficients are going in the expected direction, and the difference is significant. 



```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmFx <- lm(value.Fix ~ value.Activity+value.Social, data=loaddata)
summary(lmFx) #  Not significant
anova(lmFx)
lmFxN <- lm(value.Fix.norm ~ value.Activity+value.Social, data=loaddata)
summary(lmFxN) #  Not significant
anova(lmFxN)
# Models based on all the data -- influence of the experimental condition?
lmFix <- lm(value.Fix ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmFix) # C1 and C2 have coefficients opposed as expected...
anova(lmFix)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmFix), Sigma = vcov(lmFix), L = l)  # ... And the difference is significant (barely)!
lmFixNorm <- lm(value.Fix.norm ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmFixNorm)  # C1 and C2 have coefficients as expected
anova(lmFixNorm)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmFixNorm), Sigma = vcov(lmFixNorm), L = l)  # ... And the difference is significant!

```

* **Fix pattern**: Activity or Social are NOT significant (the normalized is the same!)
* **Contrasts**: Both in the raw and normalized Sac metric, the C1/C2 coefficients are going in the wrong direction, and the difference is significant! 

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmTh <- lm(value.Theta ~ value.Activity+value.Social, data=loaddata)
summary(lmTh) #  high load,  is low load
anova(lmTh)
lmThN <- lm(value.Theta.norm ~ value.Activity+value.Social, data=loaddata)
summary(lmThN) #  are high load,  are low load
anova(lmThN)
# Models based on all the data -- influence of the experimental condition?
lmTheta <- lm(value.Theta ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmTheta) # C1 and C2 have coefficients opposite as expected...
anova(lmTheta)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmTheta), Sigma = vcov(lmTheta), L = l)  # ... And the difference is significant
lmThetaNorm <- lm(value.Theta.norm ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmThetaNorm)  # C1 and C2 have coefficients opposite as expected
anova(lmThetaNorm)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmThetaNorm), Sigma = vcov(lmThetaNorm), L = l)  # ... And the difference is significant!

```

* **EEG theta pattern**: Activity and Social are NOT significant
* **(norm) EEG theta pattern**: Activity is significant. REP are higher value (lower load), TDT is relatively lower value (higher load)?
* **Contrasts**: In the raw and in the normalized metric, the C1/C2 coefficients are going in the opposite direction, and they are significant!


```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmJ <- lm(value.Jerk.Mean ~ value.Activity+value.Social, data=loaddata)
summary(lmJ) # MON, TDT, CLS are high load, EXP, QUE, REP, GRP is low load
anova(lmJ)
lmJN <- lm(value.Jerk.Mean.norm ~ value.Activity+value.Social, data=loaddata)
summary(lmJN) # MON, TDT, CLS are high load, EXP, QUE, REP is low load
anova(lmJN)
# Models based on all the data -- influence of the experimental condition?
lmJerk <- lm(value.Jerk.Mean ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmJerk) # C1 and C2 have coefficients  as expected...
anova(lmJerk)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmJerk), Sigma = vcov(lmJerk), L = l)  # ... And the difference is significant
lmJerkNorm <- lm(value.Jerk.Mean.norm ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmJerkNorm)  # C1 and C2 have coefficients as expected
anova(lmJerkNorm)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmJerkNorm), Sigma = vcov(lmJerkNorm), L = l)  # ... And the difference is significant!

```

* **Acc jerk pattern**: Activity and Social are significant. MON, TDT, CLS are high load, EXP, QUE, REP, GRP is low load
* **(norm) Acc jerk pattern**: Activity is significant, but Social is NOT. MON, TDT, CLS are high load, EXP, QUE, REP is low load
* **Contrasts**: In the raw and in the normalized metric, the C1/C2 coefficients are going in the expected direction, and they are significant!


So, overall, we find that some of the load metrics behave as expected by the manipulation (Acc Jerk, Sac Spd and Pupil SD - normalized), but others behave in the opposite way (Fixations, Theta, Pupil Mean)... Once we adjust for the inherent effects of Activity and Social.

Also, the different metrics give different patterns of orchestration load (see the difference between the pattern from the point of view of pupil mean and pupil SD). In most of the cases, though, the CLS is higher load than GRP (significantly)


### Multi-metric load indices. Is it better than single ones? 

#### 4 Eyetracking

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmET <- lm(FineLoad ~ value.Activity+value.Social, data=loaddataET)
summary(lmET) # MON, TDT, CLS are high load, EXP, QUE, REP, GRP is low load
anova(lmET)
# Models based on all the data -- influence of the experimental condition?
lmETFine <- lm(FineLoad ~ value.Activity+value.Social+value.Experimental, data=loaddataET)
summary(lmETFine) # C1 and C2 have coefficients  as expected...
anova(lmETFine)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmETFine), Sigma = vcov(lmETFine), L = l)  # ... And the difference is NOT significant

```

The general **orchestration patterns** with a load index based on the 4 ET metrics only, indicates that both Activity and Social are significant, and that CLS, EXP, QUE is high load, MON, GRP is low load. 

Once we correct by Activity and Social, there is no significant difference in the values of FineLoad between C1/C2 conditions.



#### 4 Eyetracking + Theta

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmETT <- lm(FineLoad ~ value.Activity+value.Social, data=loaddataETT)
summary(lmETT) 
anova(lmETT)
# Models based on all the data -- influence of the experimental condition?
lmETTFine <- lm(FineLoad ~ value.Activity+value.Social+value.Experimental, data=loaddataETT)
summary(lmETTFine) # C1 and C2 have coefficients opposite as expected...
anova(lmETTFine)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmETTFine), Sigma = vcov(lmETTFine), L = l)  # ... And the difference is NOT significant

```

The general **orchestration patterns** with a load index based on the ET+EEG metrics only, indicates that both activity and social are significant. The patterns indicate that QUE, CLS is high load, MON, GRP is low load

Once we correct by Activity and Social, there is an opposite as expected trend in FineLoad between C1/C2 conditions, but it is not significant.



#### 4 Eyetracking + Theta + Acc Jerk

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Orchestration patterns
lmETTA <- lm(FineLoad ~ value.Activity+value.Social, data=loaddataETTA)
summary(lmETTA) 
anova(lmETTA)
# Models based on all the data -- influence of the experimental condition?
lmETTAFine <- lm(FineLoad ~ value.Activity+value.Social+value.Experimental, data=loaddataETTA)
summary(lmETTAFine) # C1 and C2 have coefficients as expected...
anova(lmETTAFine)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmETTAFine), Sigma = vcov(lmETTAFine), L = l)  # ... And the difference is NOT significant

```

The general **orchestration patterns** with a load index based on the ET+EEG+ACC metrics, indicates that both activity and social are significant. The patterns are that CLS is high load, REP, GRP is low load.

Once we correct by Activity and Social, there is a difference in FineLoad between C1/C2 conditions (in the right direction), but it is not significant.


### PCA dimensions


```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Raw data PCA
# Orchestration patterns
lmPCA <- lm(Dim1 ~ value.Activity+value.Social, data=loaddata)
summary(lmPCA) #  are high load,   is low load
anova(lmPCA)
# Models based on all the data -- influence of the experimental condition?
lmPCAx <- lm(Dim1 ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmPCAx) # C1 and C2 have coefficients  as expected...
anova(lmPCAx)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmPCAx), Sigma = vcov(lmPCAx), L = l)  # ... And the difference is  significant


```

The general **orchestration patterns** with load defined by the first PCA component of the raw data, indicates that both Activity and Social are significant predictors of the main load component. CLS is high load, GRP is low load (there are also trends in activity, but these are not significant)

Once we correct by Activity and Social, there is a difference in the 1st PCA component between C1/C2 conditions (in the right direction) and it IS significant.



```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Normalized data PCA
# Orchestration patterns
lmPCA <- lm(Dim1Norm ~ value.Activity+value.Social, data=loaddata)
summary(lmPCA) #  are high load,   is low load
anova(lmPCA)
# Models based on all the data -- influence of the experimental condition?
lmPCAx <- lm(Dim1Norm ~ value.Activity+value.Social+value.Experimental, data=loaddata)
summary(lmPCAx) # C1 and C2 have coefficients  as expected...
anova(lmPCAx)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmPCAx), Sigma = vcov(lmPCAx), L = l)  # ... And the difference is  significant


```

The general **orchestration patterns** with load defined by the first PCA component of the normalized data, indicates that Activity is a significant predictor, but Social is not. REP is low load, (QUE is kind of high load, marginally significant)

Once we correct by Activity and Social, there is a difference in the 1st PCA component between C1/C2 conditions (in the right direction) IS significant.


**Would a PCA of only the ET data catch the difference??**

```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}
# Raw data PCA
res.pca = PCA(loaddataET[, c(3:6)], scale.unit=TRUE, ncp=5, graph=F)
plot.PCA(res.pca, axes=c(1, 2), choix="var", title="Raw ET data, dims 1/2")
plot.PCA(res.pca, axes=c(3, 4), choix="var", title="Raw ET data, dims 3/4")
loaddataET$Dim1 = res.pca$ind$coord[,1]
loaddataET$Dim2 = res.pca$ind$coord[,2]
loaddataET$Dim3 = res.pca$ind$coord[,3]
loaddataET$Dim4 = res.pca$ind$coord[,4]

res.pca.norm = PCA(loaddataET[, c(14,17,20,23)], scale.unit=TRUE, ncp=5, graph=F)
plot.PCA(res.pca.norm, axes=c(1, 2), choix="var", title="Normalized data, dims 1/2")
plot.PCA(res.pca.norm, axes=c(3, 4), choix="var", title="Normalized data, dims 3/4")
loaddataET$Dim1Norm = res.pca.norm$ind$coord[,1]
loaddataET$Dim2Norm = res.pca.norm$ind$coord[,2]
loaddataET$Dim3Norm = res.pca.norm$ind$coord[,3]
loaddataET$Dim4Norm = res.pca.norm$ind$coord[,4]

# Orchestration patterns
lmPCA <- lm(Dim1 ~ value.Activity+value.Social, data=loaddataET)
summary(lmPCA) #  are high load,   is low load
anova(lmPCA)
# Models based on all the data -- influence of the experimental condition?
lmPCAx <- lm(Dim1 ~ value.Activity+value.Social+value.Experimental, data=loaddataET)
summary(lmPCAx) # C1 and C2 have coefficients  as expected...
anova(lmPCAx)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmPCAx), Sigma = vcov(lmPCAx), L = l)  # ... And the difference is  significant

# Orchestration patterns
lmPCA <- lm(Dim1Norm ~ value.Activity+value.Social, data=loaddataET)
summary(lmPCA) #  are high load,   is low load
anova(lmPCA)
# Models based on all the data -- influence of the experimental condition?
lmPCAx <- lm(Dim1Norm ~ value.Activity+value.Social+value.Experimental, data=loaddataET)
summary(lmPCAx) # C1 and C2 have coefficients  as expected...
anova(lmPCAx)
l <- cbind(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -1)
wald.test(b = coef(lmPCAx), Sigma = vcov(lmPCAx), L = l)  # ... And the difference is  significant


```

BOTH the raw and normalized PCAs are able to significantly distinguish C1/C2.

The "first PCA component load" patterns for the raw PCA are CLS, QUE as high load, and MON, GRP as low load.

Oddly, the "first PCA component load" patterns for the normalized PCA are similar in terms of Activity, but OPPOSITE in the social plane (GRP is higher load) -- but given that in the normalized the pupil mean goes opposite to the main component, it is not so rare.



### HMMs


```{r, cache=TRUE, message=FALSE, warning=FALSE, echo=FALSE, fig.height=5, fig.width=5}

tabHmmAct <- table(loaddata$MMstate6,loaddata$value.Activity)
tabHmmAct
chisq.test(tabHmmAct)$p.value
chisq.test(tabHmmAct)$residuals

tabHmmSoc <- table(loaddata$MMstate6,loaddata$value.Social)
tabHmmSoc
chisq.test(tabHmmSoc)$p.value
chisq.test(tabHmmSoc)$residuals

tabHmmExp <- table(loaddata[loaddata$value.Experimental!="COM","MMstate6"],factor(loaddata[loaddata$value.Experimental!="COM","value.Experimental"]))
tabHmmExp
chisq.test(tabHmmExp)$p.value
chisq.test(tabHmmExp)$residuals

```

We can see patterns of HMM states for the different Activity and Social planes, e.g.: 

* TDT has significantly more states 5 (Mid CL, mid physical) and 6 (Low CL (PD and EEG), Low physical) and significantly less states 3 (Low CL (except EEG), Low physical) and 4 (High CL (PD and EEG), Mid physical)
* CLS has significantly more states 1 (High CL (except EEG), High physical), states 5 (Mid CL, mid physical) and 6 (Low CL (PD and EEG), Low physical) and less states 3 (Low CL (except EEG), Low physical) and 4 (High CL (PD and EEG), Mid physical)



HMMs distribution across experimental conditions is not significantly different.